{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FmVTAq-SbAE8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Sequence\n",
        "import os\n",
        "import json\n",
        "import typing\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomTokenizer:\n",
        "  ## this class is used to tokenize and detokenize the sentence\n",
        "\n",
        "  ##\n",
        "  def __init__(self,\n",
        "               split: str=\" \",\n",
        "               char_level: bool=False,\n",
        "               lower:bool=True,\n",
        "               start_token:str=\"<start>\",\n",
        "               end_token:str=\"<end>\",\n",
        "               filters: list = ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'],\n",
        "               filter_nums: bool = True,\n",
        "               start: int=1\n",
        "               ):\n",
        "    self.split=split\n",
        "    self.char_level=char_level\n",
        "    self.lower=lower\n",
        "    self.start_token=start_token\n",
        "    self.end_token=end_token\n",
        "    self.filters=filters\n",
        "    self.filter_nums=filter_nums\n",
        "    self.start=start\n",
        "    self.max_length=0\n",
        "    self.word_index={}\n",
        "    self.index_word={}\n",
        "\n",
        "  def start_token_index(self):\n",
        "    return self.word_index[self.start_token]\n",
        "\n",
        "  def end_token_index(self):\n",
        "    return self.word_index[self.end_token]\n",
        "\n",
        "  def sort(self):\n",
        "\n",
        "    ## for creating sorted word_index and index_word dictionary\n",
        "\n",
        "    all_words = sorted(list(self.word_index.keys()))\n",
        "\n",
        "        # Reconstruct word_index and index_word with new sequential indices\n",
        "    self.word_index = {}\n",
        "    self.index_word = {}\n",
        "    current_index = self.start\n",
        "    for word in all_words:\n",
        "      self.word_index[word] = current_index\n",
        "      self.index_word[current_index] = word\n",
        "      current_index += 1\n",
        "\n",
        "\n",
        "  def split_line(self,line:str):\n",
        "     ## split the line into words and special characters\n",
        "\n",
        "     if self.lower:\n",
        "      line=line.lower()\n",
        "     else:\n",
        "      line=line\n",
        "\n",
        "\n",
        "     if self.char_level:\n",
        "      return [char for char in line]\n",
        "\n",
        "     line_token=line.split(self.split)\n",
        "\n",
        "\n",
        "     new_token=[]\n",
        "\n",
        "     for index,token in enumerate(line_token):\n",
        "      new_tokens=[''];\n",
        "      for char_index,char in enumerate(token):\n",
        "        if (char in self.filters) or (self.filter_nums and char.isdigit()):\n",
        "          if(len(token)-1!=char_index):\n",
        "            new_tokens+=[char,'']\n",
        "          else:\n",
        "            new_tokens+=[char]\n",
        "        else:\n",
        "          new_tokens[-1]+=[char]\n",
        "\n",
        "        new_token+=new_tokens\n",
        "        if len(line_token)-1 !=index:\n",
        "          new_token+=[self.split]\n",
        "\n",
        "     new_token=[token for token in new_token if token !='']\n",
        "\n",
        "     return new_token\n",
        "\n",
        "\n",
        "  def fit_on_texts(self,text:typing.List[str]):\n",
        "    ## it fit the tokenizer on the list of lines\n",
        "\n",
        "    ## update the word_index and index_word dictionary based on the text\n",
        "\n",
        "    self.word_index={word:index for index,word in enumerate([self.start_token,self.end_token,self.split]+self.filters)}\n",
        "\n",
        "    for line in tqdm(text,desc=\"fitting tokenizer\"):\n",
        "      line_tokens=self.split_line(line)\n",
        "      self.max_length=max(self.max_length,len(line_tokens)+2)\n",
        "      for token in line_tokens:\n",
        "\n",
        "        if token not in self.word_index:\n",
        "          self.word_index[token]=len(self.word_index)\n",
        "\n",
        "    self.sort()\n",
        "\n",
        "  def update(self, lines: typing.List[str]):\n",
        "        \"\"\" Updates the tokenizer with new lines of text\n",
        "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
        "\n",
        "        Args:\n",
        "            lines (typing.List[str]): List of lines of text to update the tokenizer with\n",
        "        \"\"\"\n",
        "        new_tokens = 0\n",
        "        for line in tqdm(lines, desc=\"Updating tokenizer\"):\n",
        "            line_tokens = self.split_line(line)\n",
        "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
        "            for token in line_tokens:\n",
        "                if token not in self.word_index:\n",
        "                    self.word_index[token] = len(self.word_index)\n",
        "                    new_tokens += 1\n",
        "\n",
        "        self.sort()\n",
        "        print(f\"Added {new_tokens} new tokens\")\n",
        "\n",
        "  def detokenizer(self,sequences:typing.List[int],remove_start_end:bool=True):\n",
        "\n",
        "  ## this is used to detokenize the list of integers into the considerble text\n",
        "\n",
        "  ## return the list of the word\n",
        "    lines=[]\n",
        "    for sequence in sequences:\n",
        "      line=\"\"\n",
        "      for token in sequence:\n",
        "        if token==0:\n",
        "          break\n",
        "        if remove_start_end and (token==self.start_token_index() or token==self.end_token_index()):\n",
        "          continue\n",
        "        line+=self.index_word[token]\n",
        "\n",
        "      lines.append(line)\n",
        "    return lines\n",
        "\n",
        "\n",
        "  def text_to_sequences(self,text:typing.List[str],add_start_end:bool=True):\n",
        "  ## this is used to convert the  text into there considerable token\n",
        "\n",
        "  ## this function return the list of integer\n",
        "\n",
        "    Sequences=[]\n",
        "    for lines in text:\n",
        "      line=self.split_line(lines)\n",
        "      Sequence=[self.word_index[word] for word in line if word in self.word_index]\n",
        "      if add_start_end:\n",
        "        Sequence= [self.word_index[self.start_token]] + Sequence + [self.word_index[self.end_token]]\n",
        "      Sequences.append(Sequence)\n",
        "    return Sequences\n",
        "\n",
        "\n",
        "  def save(self, path: str, type: str=\"json\"):\n",
        "        \"\"\" Saves the tokenizer to a file\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to save the tokenizer to\n",
        "            type (str, optional): Type of file to save the tokenizer to. Defaults to \"json\".\n",
        "        \"\"\"\n",
        "        serialised_dict = self.dict()\n",
        "        if type == \"json\":\n",
        "            if os.path.dirname(path):\n",
        "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(serialised_dict, f)\n",
        "\n",
        "  def dict(self):\n",
        "        \"\"\" Returns a dictionary of the tokenizer\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary of the tokenizer\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"split\": self.split,\n",
        "            \"lower\": self.lower,\n",
        "            \"char_level\": self.char_level,\n",
        "            \"index_word\": self.index_word,\n",
        "            \"max_length\": self.max_length,\n",
        "            \"start_token\": self.start_token,\n",
        "            \"end_token\": self.end_token,\n",
        "            \"filters\": self.filters,\n",
        "            \"filter_nums\": self.filter_nums,\n",
        "            \"start\": self.start\n",
        "        }\n",
        "\n",
        "  @staticmethod\n",
        "  def load(path: typing.Union[str, dict], type: str=\"json\"):\n",
        "        \"\"\" Loads a tokenizer from a file\n",
        "\n",
        "        Args:\n",
        "            path (typing.Union[str, dict]): Path to load the tokenizer from or a dictionary of the tokenizer\n",
        "            type (str, optional): Type of file to load the tokenizer from. Defaults to \"json\".\n",
        "\n",
        "        Returns:\n",
        "            CustomTokenizer: Loaded tokenizer\n",
        "        \"\"\"\n",
        "        if isinstance(path, str):\n",
        "            if type == \"json\":\n",
        "                with open(path, \"r\") as f:\n",
        "                    load_dict = json.load(f)\n",
        "\n",
        "        elif isinstance(path, dict):\n",
        "            load_dict = path\n",
        "\n",
        "        tokenizer = CustomTokenizer()\n",
        "        tokenizer.split = load_dict[\"split\"]\n",
        "        tokenizer.lower = load_dict[\"lower\"]\n",
        "        tokenizer.char_level = load_dict[\"char_level\"]\n",
        "        # Ensure index_word keys are integers upon loading\n",
        "        tokenizer.index_word = {int(k): v for k, v in load_dict[\"index_word\"].items()}\n",
        "        tokenizer.max_length = load_dict[\"max_length\"]\n",
        "        tokenizer.start_token = load_dict[\"start_token\"]\n",
        "        tokenizer.end_token = load_dict[\"end_token\"]\n",
        "        tokenizer.filters = load_dict[\"filters\"]\n",
        "        tokenizer.filter_nums = bool(load_dict[\"filter_nums\"])\n",
        "        tokenizer.start = load_dict[\"start\"]\n",
        "        # Reconstruct word_index from the loaded index_word\n",
        "        tokenizer.word_index = {v: k for k, v in tokenizer.index_word.items()}\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.index_word)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vz_iMYSlbSJq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path='/content/eng_-french.csv.zip'\n",
        "with zipfile.ZipFile(zip_path,'r') as zip_ref:\n",
        "  zip_ref.extractall('tokenizer_eng_to_french')"
      ],
      "metadata": {
        "id": "g103Q-L2RPeJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for i in os.listdir('/content/tokenizer_eng_to_french'):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvfV3a5WRnDh",
        "outputId": "542e8000-a323-435c-e604-f0da979fe2f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eng_-french.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/tokenizer_eng_to_french/eng_-french.csv')"
      ],
      "metadata": {
        "id": "4PtToxw0SBQS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df=df.head(500)"
      ],
      "metadata": {
        "id": "wGVmLBP9SFF5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RF5zuK6-SK5c",
        "outputId": "7d48b072-3ab7-4154-91f1-beb3d5db8d36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    English words/sentences          French words/sentences\n",
              "264               Get lost!  Va voir ailleurs si j'y suis !\n",
              "366               I'm safe.            Je suis en sécurité.\n",
              "28                  Cheers!                         Merci !\n",
              "282               He tries.                      Il essaye.\n",
              "21                   I won!               Je l'ai emporté !"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8081bc9-6292-4889-960b-8be0c0c8c82b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>Get lost!</td>\n",
              "      <td>Va voir ailleurs si j'y suis !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>I'm safe.</td>\n",
              "      <td>Je suis en sécurité.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Cheers!</td>\n",
              "      <td>Merci !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>He tries.</td>\n",
              "      <td>Il essaye.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>I won!</td>\n",
              "      <td>Je l'ai emporté !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8081bc9-6292-4889-960b-8be0c0c8c82b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8081bc9-6292-4889-960b-8be0c0c8c82b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8081bc9-6292-4889-960b-8be0c0c8c82b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c8ecdc32-3bd3-41b7-ae20-f18957a8fb34\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8ecdc32-3bd3-41b7-ae20-f18957a8fb34')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c8ecdc32-3bd3-41b7-ae20-f18957a8fb34 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"English words/sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I'm safe.\",\n          \"I won!\",\n          \"Cheers!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French words/sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Je suis en s\\u00e9curit\\u00e9.\",\n          \"Je l'ai emport\\u00e9 !\",\n          \"Merci !\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng=df['English words/sentences']"
      ],
      "metadata": {
        "id": "pZpj7gsuSOVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train=new_df['English words/sentences'].tolist()"
      ],
      "metadata": {
        "id": "woctCiWwUhOd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fre_train=new_df['French words/sentences'].tolist()"
      ],
      "metadata": {
        "id": "Eed1L7KVirSK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = CustomTokenizer(char_level=True)\n",
        "tokenize.fit_on_texts(eng_train)\n",
        "tokenize.save(\"tokenizer_eng.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X7R9NiDTQV7",
        "outputId": "7a76f6c6-dd9a-493f-fcc3-10c7c6e20604"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fitting tokenizer: 100%|██████████| 500/500 [00:00<00:00, 606463.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize1=CustomTokenizer(char_level=True)\n",
        "tokenize1.fit_on_texts(fre_train)\n",
        "tokenize1.save(\"tokenizer_fre.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qwbdCRxYLNw",
        "outputId": "76c4ad19-69aa-4f8b-9fe3-dd449368ced7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fitting tokenizer: 100%|██████████| 500/500 [00:00<00:00, 208216.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_sen=tokenize.text_to_sequences([\"Hang on.\"])[0]\n",
        "tokenize_sen2=tokenize.text_to_sequences([\"Get lost!\"])[0]\n",
        "print(tokenize_sen)\n",
        "print(tokenize_sen2)\n",
        "\n",
        "detokinze_sen=tokenize.detokenizer([tokenize_sen],remove_start_end=False)\n",
        "detokinze_sen2=tokenize.detokenizer([tokenize_sen2],remove_start_end=False)\n",
        "print(detokinze_sen)\n",
        "print(detokinze_sen2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f1wjkG4i2hl",
        "outputId": "f4cb1fd2-d784-4367-8b4a-a2b51a3cf14e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25, 43, 36, 49, 42, 3, 50, 49, 17, 24]\n",
            "[25, 42, 40, 55, 3, 47, 50, 54, 55, 4, 24]\n",
            "['<start>hang on.<end>']\n",
            "['<start>get lost!<end>']\n"
          ]
        }
      ]
    }
  ]
}